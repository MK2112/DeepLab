{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks - Nowhere Near A Relic\n",
    "\n",
    "Recurrent Neural Networks (RNNs) have long been a staple in the processing of sequential data, be it in natural language or computer vision. However, the recent success of Transformers in the field of Natural Language Processing (NLP) has led to a decline in the use of RNNs. This is because Transformers are able to process sequences in parallel, as opposed to RNNs which process sequences sequentially.Additionally, classic RNNs suffer from a phenomenon limiting their capabilities. Even though possible in theory, RNNs turn out to be incapable of recognizing or learning long-term dependencies. But successor architectures addressed this. \n",
    "\n",
    "Let us take a look at how RNNs work, why their application has its boundaries and how these were dealt with over the years.<br>\n",
    "Finally, let us explore the current developments around sequence learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
